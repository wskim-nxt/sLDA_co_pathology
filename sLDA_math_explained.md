# Supervised LDA for Co-Pathology: Mathematical Foundations

## Core Concept

**Goal:** Discover latent pathology patterns (topics) from brain atrophy data while simultaneously predicting diagnosis.

**Key Idea:** Each patient has a **mixture of pathology patterns** (not just one pure disease). sLDA discovers these patterns AND links them to clinical diagnoses.

---

## Why sLDA for Co-Pathology?

### Traditional Approach Problems
- **Single diagnosis assumption:** Each patient labeled as AD, PD, or DLB
- **Reality:** Many patients have **overlapping pathologies** (e.g., AD + vascular, DLB = AD + Lewy bodies)
- **Lost information:** Can't capture the continuum of mixed presentations

### sLDA Solution
- **Topic mixture:** Each patient = combination of multiple pathology patterns
- **Supervised learning:** Topics must be useful for predicting diagnosis
- **Interpretability:** Topics = regional atrophy profiles we can understand

**Example Patient:**
```
Patient_123:
  - 60% Topic 0 (limbic atrophy â†’ AD-like)
  - 30% Topic 1 (cortical atrophy â†’ DLB-like)
  - 10% Topic 2 (minimal atrophy â†’ HC-like)

â†’ Mixed AD-DLB pathology captured!
```

---

## Mathematical Framework

### The Generative Story

Imagine how a patient's brain atrophy data is "generated":

1. **Nature picks pathology patterns** (topics exist in the world)
2. **Each patient gets a mixture** of these patterns (Î¸)
3. **Atrophy values emerge** from this mixture (X)
4. **Diagnosis follows** from the mixture (y)

---

## Core Variables

| Symbol | Name | Dimensions | Meaning |
|--------|------|------------|---------|
| **D** | Patients | 209 | Number of subjects |
| **V** | Features | 62 | Number of brain regions (cortical) |
| **K** | Topics | 4 | Number of pathology patterns |
| **C** | Classes | 5 | Number of diagnoses (AD, PD, DLB, SVAD, HC) |
| **Î²** | Topic patterns | K Ã— V | What regions are affected in each pattern |
| **Î¸** | Patient mixtures | D Ã— K | How much of each pattern each patient has |
| **Î·** | Diagnosis weights | K Ã— C | How topics predict diagnoses |
| **X** | Atrophy data | D Ã— V | Observed regional volumes |
| **y** | Diagnoses | D | Observed diagnosis labels |

---

## The sLDA Model (Full Math)

### 1. Topic Patterns (Î²)

**What:** Each topic defines a regional atrophy signature.

**Math:**
```
For each topic k = 1, ..., K:
  For each region v = 1, ..., V:
    Î²_kv ~ Normal(0, Ïƒ_Î²Â²)
```

**Code:** [slda_model.py:125-131](slda_model.py:125-131)
```python
# ---- Topic-specific atrophy patterns ----
# Each topic k has a mean atrophy value for each of V regions
# Shape: (K topics, V features)
beta = pm.Normal(
    "beta",
    mu=0.0,
    sigma=self.feature_prior_std,
    shape=(self.n_topics, self.n_features_)
)
```

**Interpretation:**
- `Î²[0, :]` = Topic 0's atrophy profile across 62 regions
- `Î²[0, 5] = +0.8` means region 5 has high atrophy in Topic 0
- `Î²[1, 5] = -0.3` means region 5 is relatively preserved in Topic 1

---

### 2. Patient Topic Mixtures (Î¸)

**What:** Each patient has a probability distribution over topics.

**Math:**
```
For each patient d = 1, ..., D:
  Î¸_d ~ Dirichlet(Î±)

where:
  Î¸_d = [Î¸_d1, Î¸_d2, ..., Î¸_dK]
  Î£_k Î¸_dk = 1  (proportions sum to 1)
  Î¸_dk â‰¥ 0      (non-negative)
```

**Code:** [slda_model.py:133-139](slda_model.py:133-139)
```python
# ---- Patient-topic mixtures ----
# Each patient d has a mixture over K topics
# Shape: (D patients, K topics)
# Constraint: each row sums to 1
theta = pm.Dirichlet(
    "theta",
    a=alpha,
    shape=(self.n_patients_, self.n_topics)
)
```

**The Dirichlet Distribution:**
- **Prior:** `Î± = [1, 1, 1, 1]` (uniform prior, no preference)
- **Effect:** Patient can have ANY mixture of topics
- **Example outputs:**
  - Pure: `[0.95, 0.02, 0.02, 0.01]` â†’ mostly Topic 0
  - Mixed: `[0.40, 0.35, 0.20, 0.05]` â†’ co-pathology!

---

### 3. Atrophy Likelihood (Continuous Features)

**What:** Patient's observed atrophy is a weighted combination of topic patterns.

**Math:**
```
For each patient d and region v:
  x_dv ~ Normal(Î¼_dv, Ïƒ_xÂ²)

where:
  Î¼_dv = Î£_k Î¸_dk Ã— Î²_kv = (Î¸_d âŠ— Î²)_v
```

**Code:** [slda_model.py:141-158](slda_model.py:141-158)
```python
# ---- Likelihood for regional atrophy values ----
# For each patient, observed atrophy is a mixture of topic patterns
# x_d ~ Normal(theta_d @ beta, sigma)
# This creates a weighted combination of topic patterns

# Compute expected atrophy for each patient-region pair
# Shape: (D patients, V features)
mu_x = pm.math.dot(theta, beta)

# Observation noise
sigma_x = pm.HalfNormal("sigma_x", 1.0)

# Observed atrophy values
x_obs = pm.Normal(
    "x_obs",
    mu=mu_x,
    sigma=sigma_x,
    observed=X
)
```

**Key Difference from Text LDA:**
- **Text LDA:** Words are discrete counts â†’ Multinomial likelihood
- **Our sLDA:** Atrophy is continuous â†’ **Normal likelihood**

**Intuition:**
```
Patient with Î¸ = [0.6, 0.4, 0.0, 0.0]:

  Expected atrophy in region v:
  Î¼_v = 0.6 Ã— Î²[0,v] + 0.4 Ã— Î²[1,v]
      = 0.6 Ã— (limbic pattern) + 0.4 Ã— (cortical pattern)

  Observed:
  x_v ~ Normal(Î¼_v, Ïƒ_x)
```

---

### 4. Supervised Component (Diagnosis Prediction)

**What:** Topics predict diagnosis via softmax regression.

**Math:**
```
For each patient d:
  logit_dc = Î£_k Î¸_dk Ã— Î·_kc

  P(y_d = c | Î¸_d) = exp(logit_dc) / Î£_c' exp(logit_dc')  [Softmax]

  y_d ~ Categorical(P(y_d | Î¸_d))
```

**Code:** [slda_model.py:160-175](slda_model.py:160-175)
```python
# ---- Supervised component: Topic â†’ Diagnosis ----
# Linear combination of topics predicts diagnosis
# Shape: (K topics, C classes)
eta = pm.Normal(
    "eta",
    mu=0.0,
    sigma=2.0,
    shape=(self.n_topics, self.n_classes_)
)

# For each patient, compute class logits from topic mixture
# Shape: (D patients, C classes)
logits = pm.math.dot(theta, eta)

# Categorical likelihood for diagnosis
y_obs = pm.Categorical(
    "y_obs",
    logit_p=logits,
    observed=y
)
```

**Interpretation of Î·:**
```
Î· = [
  # AD   PD   DLB  SVAD  HC
  [+2.3, -0.5, +0.8, +1.2, -3.1],  # Topic 0 (limbic)
  [+0.9, -0.3, +2.1, +0.4, -2.5],  # Topic 1 (cortical)
  [-0.5, +1.8, +0.6, -0.2, -1.2],  # Topic 2 (frontal)
  [-2.8, -1.5, -2.3, -2.1, +4.5],  # Topic 3 (preservation)
]

Topic 0 â†’ strongly predicts AD (+2.3)
Topic 3 â†’ strongly predicts HC (+4.5)
```

**Why This Works:**
- **Force topics to be useful:** They must predict diagnosis
- **But allow flexibility:** Topics can predict multiple diagnoses
- **Captures overlap:** DLB-predicting topic might also help predict AD

---

## Complete Model Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GENERATIVE PROCESS                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. Nature creates K pathology patterns:                    â”‚
â”‚     Î² ~ Normal(0, ÏƒÂ²)                    [K Ã— V matrix]     â”‚
â”‚                                                              â”‚
â”‚  2. Each patient gets a topic mixture:                      â”‚
â”‚     Î¸_d ~ Dirichlet(Î±)                   [K-dim vector]     â”‚
â”‚                                                              â”‚
â”‚  3. Atrophy emerges from mixture:                           â”‚
â”‚     x_dv ~ Normal(Î¸_d @ Î²_v, Ïƒ_xÂ²)       [D Ã— V matrix]     â”‚
â”‚                                                              â”‚
â”‚  4. Diagnosis follows from mixture:                         â”‚
â”‚     y_d ~ Categorical(Softmax(Î¸_d @ Î·))  [D-dim vector]     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Inference: Learning from Data

**Given:** Observed data (X, y)
**Learn:** Hidden variables (Î², Î¸, Î·)

**Method:** Bayesian inference via MCMC (NUTS sampler)

**Code:** [slda_model.py:177-185](slda_model.py:177-185)
```python
# ---- MCMC Sampling ----
print("\nStarting MCMC sampling...")
self.trace_ = pm.sample(
    draws=n_samples,
    tune=tune,
    chains=chains,
    target_accept=target_accept,
    random_seed=self.random_state,
    return_inferencedata=True,
    **kwargs
)
```

**What MCMC Does:**
1. Starts with random Î², Î¸, Î·
2. Proposes changes that increase P(Î², Î¸, Î· | X, y)
3. Samples from posterior distribution
4. Returns uncertainty: not just point estimates!

**Output:**
- `posterior["beta"]`: 2000 samples Ã— 4 chains Ã— (K Ã— V) matrix
- We use the mean: `Î²Ì‚ = mean(posterior["beta"])`

---

## Key Mathematical Properties

### 1. Identifiability
**Problem:** Î² and Î¸ are not uniquely identified (label switching).
**Solution:** Supervised component (Î·) breaks symmetry by linking topics to diagnoses.

### 2. Topic Mixtures Sum to 1
```python
theta = pm.Dirichlet(...)  # Ensures Î£_k Î¸_dk = 1
```
This makes Î¸ interpretable as proportions.

### 3. Normal Likelihood Choice
**Why Normal?**
- Atrophy values are continuous (z-scored volumes)
- Already normalized/standardized
- Symmetric around zero

**Alternative:** Could use Lognormal if values are strictly positive and right-skewed.

---

## Example Walkthrough

Let's trace through one patient:

**Patient 42:**
- Observed: `X[42, :] = [0.5, 0.8, -0.2, ..., 0.3]` (62 values)
- Diagnosis: `y[42] = 0` (AD)

**Step 1: Model learns topics**
```python
Î²[0, :] = [0.9, 0.7, 0.1, ..., 0.4]  # Limbic pattern
Î²[1, :] = [0.2, 0.3, 0.8, ..., 0.6]  # Cortical pattern
Î²[2, :] = [-0.1, 0.1, 0.4, ..., 0.2] # Frontal pattern
Î²[3, :] = [-0.5, -0.4, -0.2, ..., -0.3] # Preservation
```

**Step 2: Model infers mixture**
```python
Î¸[42, :] = [0.65, 0.25, 0.08, 0.02]
# 65% limbic, 25% cortical, 8% frontal, 2% preservation
```

**Step 3: Predicted atrophy**
```python
Î¼[42, :] = Î¸[42, :] @ Î²
         = 0.65 Ã— Î²[0,:] + 0.25 Ã— Î²[1,:] + 0.08 Ã— Î²[2,:] + 0.02 Ã— Î²[3,:]

For region 0:
Î¼[42, 0] = 0.65 Ã— 0.9 + 0.25 Ã— 0.2 + 0.08 Ã— (-0.1) + 0.02 Ã— (-0.5)
         = 0.585 + 0.05 - 0.008 - 0.01
         = 0.617

Observed: X[42, 0] = 0.5
Likelihood: P(X[42,0] | ...) = Normal(0.5 | Î¼=0.617, Ïƒ=0.1)
```

**Step 4: Predicted diagnosis**
```python
logits[42, :] = Î¸[42, :] @ Î·
              = 0.65 Ã— Î·[0,:] + 0.25 Ã— Î·[1,:] + ...

For AD (class 0):
logits[42, 0] = 0.65 Ã— 2.3 + 0.25 Ã— 0.9 + 0.08 Ã— (-0.5) + 0.02 Ã— (-2.8)
              = 1.495 + 0.225 - 0.04 - 0.056
              = 1.624

P(AD | Î¸[42]) = exp(1.624) / (exp(1.624) + exp(-0.3) + ... + exp(-2.1))
              â‰ˆ 0.72  (72% probability)

True diagnosis: AD âœ“
```

---

## Why This Model Captures Co-Pathology

### Traditional Classification
```
Input: X â†’ Model â†’ Output: "AD" or "PD" or "DLB"
```
**Problem:** Binary decision, no mixed pathology information.

### Our sLDA Model
```
Input: X â†’ Model â†’ Output:
  - Î¸ = [0.65, 0.25, 0.08, 0.02]  â† Co-pathology mixture!
  - P(AD) = 0.72, P(DLB) = 0.18, ...  â† Uncertainty
  - Predicted: AD
```

**Benefits:**
1. **Mixture visible:** We see 65% AD-like + 25% DLB-like
2. **Interpretable:** Topics are regional atrophy patterns
3. **Uncertainty:** Probability distribution over diagnoses
4. **Clinical utility:** "This patient has mixed AD-DLB pathology"

---

## Comparison: Text sLDA vs. Our sLDA

| Aspect | Text sLDA | Our Co-Pathology sLDA |
|--------|-----------|------------------------|
| **Documents** | Movie reviews | Patients |
| **Words** | Vocabulary words | Brain regions |
| **Word values** | Counts (discrete) | Atrophy (continuous) |
| **Topics** | Themes (e.g., "action", "romance") | Pathology patterns (e.g., "limbic", "cortical") |
| **Response** | Sentiment (1-5 stars) | Diagnosis (AD, PD, etc.) |
| **Likelihood** | Multinomial | **Normal** |
| **Supervised** | Linear regression | **Softmax classification** |
| **Topic mixture** | Document about multiple themes | **Patient with multiple pathologies** |

**Key Innovation:** Adapted LDA for continuous neuroimaging + categorical diagnosis.

---

## References

### Original sLDA Paper
Blei, D. M., & McAuliffe, J. D. (2007). **Supervised topic models**. *Advances in Neural Information Processing Systems*, 20.

**Our adaptation:**
- Continuous likelihood for brain atrophy (Normal vs Multinomial)
- Categorical outcome for diagnosis (Softmax vs Linear)
- Applied to co-pathology discovery in neurodegenerative diseases

---

## Summary

**sLDA discovers latent pathology patterns by:**
1. **Decomposing** patient atrophy into topic mixtures (Î¸)
2. **Learning** what regions define each pattern (Î²)
3. **Linking** patterns to diagnoses (Î·)
4. **Capturing** co-pathology through mixed topic membership

**Math in one line:**
```
P(X, y | Î², Î¸, Î·) = âˆ_d [ âˆ_v Normal(x_dv | Î¸_d @ Î²_v, ÏƒÂ²) Ã— Categorical(y_d | Softmax(Î¸_d @ Î·)) ]
```

**Clinical insight:**
> "This patient has 60% AD-like limbic atrophy and 30% DLB-like cortical atrophy, explaining their mixed clinical presentation."

That's the power of sLDA for co-pathology analysis! ðŸ§ 
