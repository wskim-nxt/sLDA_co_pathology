{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised LDA for Co-Pathology Analysis\n",
    "\n",
    "This notebook demonstrates how to use the supervised Latent Dirichlet Allocation (sLDA) model to discover co-pathology patterns in neurodegenerative diseases from regional gray matter atrophy data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Goal**: Identify latent pathology patterns (topics) that explain regional atrophy and predict diagnosis\n",
    "- **Data**: 209 patients with 5 diagnoses (AD, PD, DLB, SVAD, HC) and 62 cortical region measurements\n",
    "- **Model**: sLDA with continuous Normal likelihood for features and Categorical likelihood for diagnosis\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Topics** = Latent pathology patterns (e.g., limbic atrophy, cortical atrophy)\n",
    "- **Patient mixtures** = Each patient has a distribution over topics (co-pathology)\n",
    "- **Topic patterns** = Each topic has characteristic regional atrophy signatures\n",
    "- **Supervised component** = Topics predict diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our custom modules\n",
    "from preprocessing import load_wsev_data, prepare_slda_inputs, train_test_split_stratified\n",
    "from slda_model import CoPathologySLDA\n",
    "from visualization import (\n",
    "    plot_topic_heatmap,\n",
    "    plot_patient_topic_distribution,\n",
    "    plot_topic_diagnosis_association,\n",
    "    plot_brain_topic_pattern,\n",
    "    plot_copathology_mixtures,\n",
    "    plot_confusion_matrix\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 160 patients from /home/coder/data/updated_WSEV/260108_wsev_final_df.csv\n",
      "Columns: 113\n",
      "\n",
      "Dataset shape: (160, 113)\n",
      "\n",
      "Diagnosis distribution:\n",
      "DX\n",
      "PD      56\n",
      "AD      55\n",
      "DLB     28\n",
      "SVAD    21\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the WSEV dataset\n",
    "data_path = '/home/coder/data/updated_WSEV/260108_wsev_final_df.csv'\n",
    "df = load_wsev_data(data_path)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nDiagnosis distribution:\")\n",
    "print(df['DX'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis distribution:\n",
      "  AD: 55 patients (class 0)\n",
      "  DLB: 28 patients (class 1)\n",
      "  PD: 56 patients (class 2)\n",
      "  SVAD: 21 patients (class 3)\n",
      "\n",
      "Final data shape:\n",
      "  X: (160, 95) (patients × cortical regions)\n",
      "  y: (160,) (patients,)\n",
      "  Features: 95\n",
      "  Diagnoses: 4\n",
      "\n",
      "Feature matrix X: (160, 95)\n",
      "Diagnosis labels y: (160,)\n",
      "Number of features: 95\n",
      "Diagnoses: ['AD', 'DLB', 'PD', 'SVAD']\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for sLDA\n",
    "X, y, feature_names, dx_labels = prepare_slda_inputs(df, standardize=False)\n",
    "\n",
    "print(f\"\\nFeature matrix X: {X.shape}\")\n",
    "print(f\"Diagnosis labels y: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Diagnoses: {dx_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Split into train/test sets\n",
    "# For this example, we'll train on all data for better topic discovery\n",
    "# Uncomment below to use train/test split:\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split_stratified(X, y, test_size=0.2)\n",
    "# X_model, y_model = X_train, y_train\n",
    "\n",
    "# For full dataset:\n",
    "X_model, y_model = X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training\n",
    "\n",
    "We'll fit the sLDA model with 4 topics. This may take several minutes depending on your system.\n",
    "\n",
    "**Note**: Adjust `n_topics` to explore different numbers of pathology patterns (3-6 recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 4 topics\n",
      "\n",
      "Note: Sampling may take 5-15 minutes...\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = CoPathologySLDA(\n",
    "    n_topics=4,           # Number of latent pathology patterns\n",
    "    alpha_prior=1.0,      # Dirichlet concentration (1.0 = uniform)\n",
    "    feature_prior_std=1.0, # Prior std for topic-region weights\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Model initialized with 4 topics\")\n",
    "print(\"\\nNote: Sampling may take 5-15 minutes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting sLDA model:\n",
      "  Patients: 160\n",
      "  Features: 95\n",
      "  Topics: 4\n",
      "  Diagnoses: 4\n",
      "  Sampling: 2000 samples × 4 chains\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pymc' has no attribute 'Constant'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# For faster testing, reduce n_samples to 500 and chains to 2\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# For final analysis, use n_samples=2000 and chains=4\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Number of MCMC samples per chain\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Number of tuning/burn-in samples\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchains\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Number of parallel chains\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Target acceptance rate for NUTS\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sLDA_co_pathology/slda_model.py:115\u001b[39m, in \u001b[36mCoPathologySLDA.fit\u001b[39m\u001b[34m(self, X, y, n_samples, tune, chains, target_accept, **kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Sampling: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples × \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchains\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chains\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pm.Model() \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# ---- Hyperpriors ----\u001b[39;00m\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# Dirichlet concentration for patient-topic mixtures\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     alpha = \u001b[43mpm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConstant\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.alpha_prior * np.ones(\u001b[38;5;28mself\u001b[39m.n_topics))\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# ---- Topic-specific atrophy patterns ----\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# Each topic k has a mean atrophy value for each of V regions\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# Shape: (K topics, V features)\u001b[39;00m\n\u001b[32m    120\u001b[39m     beta = pm.Normal(\n\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    122\u001b[39m         mu=\u001b[32m0.0\u001b[39m,\n\u001b[32m    123\u001b[39m         sigma=\u001b[38;5;28mself\u001b[39m.feature_prior_std,\n\u001b[32m    124\u001b[39m         shape=(\u001b[38;5;28mself\u001b[39m.n_topics, \u001b[38;5;28mself\u001b[39m.n_features_)\n\u001b[32m    125\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pymc' has no attribute 'Constant'"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "# For faster testing, reduce n_samples to 500 and chains to 2\n",
    "# For final analysis, use n_samples=2000 and chains=4\n",
    "\n",
    "model.fit(\n",
    "    X_model, \n",
    "    y_model,\n",
    "    n_samples=2000,      # Number of MCMC samples per chain\n",
    "    tune=1000,           # Number of tuning/burn-in samples\n",
    "    chains=4,            # Number of parallel chains\n",
    "    target_accept=0.9    # Target acceptance rate for NUTS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Model Parameters\n",
    "\n",
    "After fitting, we extract the posterior means of key parameters:\n",
    "- **β (beta)**: Topic-region patterns (K × V matrix)\n",
    "- **θ (theta)**: Patient-topic mixtures (D × K matrix)\n",
    "- **η (eta)**: Topic-diagnosis associations (K × C matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get posterior means\n",
    "topic_patterns = model.get_topic_patterns()        # (n_topics, n_features)\n",
    "patient_mixtures = model.get_patient_mixtures()    # (n_patients, n_topics)\n",
    "diagnosis_weights = model.get_diagnosis_weights()  # (n_topics, n_classes)\n",
    "\n",
    "print(f\"Topic patterns (β): {topic_patterns.shape}\")\n",
    "print(f\"Patient mixtures (θ): {patient_mixtures.shape}\")\n",
    "print(f\"Diagnosis weights (η): {diagnosis_weights.shape}\")\n",
    "\n",
    "# Verify topic mixtures sum to 1\n",
    "print(f\"\\nPatient mixture sums (should be 1.0): {patient_mixtures[0].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Topic Patterns\n",
    "\n",
    "### 4.1 Topic-Region Heatmap\n",
    "\n",
    "Shows which brain regions are associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_topic_heatmap(\n",
    "    topic_patterns, \n",
    "    feature_names,\n",
    "    figsize=(16, 6),\n",
    "    save_path='topic_heatmap.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Top Regions per Topic\n",
    "\n",
    "Identify the most important regions for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top regions for each topic\n",
    "for topic_id in range(model.n_topics):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Topic {topic_id} - Top 10 Regions\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    top_regions = model.get_topic_top_regions(\n",
    "        topic_id, \n",
    "        feature_names, \n",
    "        n_regions=10,\n",
    "        absolute=True\n",
    "    )\n",
    "    \n",
    "    for i, (region, weight) in enumerate(top_regions, 1):\n",
    "        print(f\"{i:2d}. {region:40s} {weight:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Detailed Topic Visualization\n",
    "\n",
    "Visualize top regions for each topic, separated by hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize each topic\n",
    "for topic_id in range(model.n_topics):\n",
    "    fig = plot_brain_topic_pattern(\n",
    "        topic_id,\n",
    "        topic_patterns,\n",
    "        feature_names,\n",
    "        n_top_regions=12,\n",
    "        figsize=(14, 5),\n",
    "        save_path=f'topic_{topic_id}_brain_pattern.png'\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Patient Topic Mixtures\n",
    "\n",
    "### 5.1 Topic Distribution by Diagnosis\n",
    "\n",
    "Shows how each diagnosis group loads on different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_patient_topic_distribution(\n",
    "    patient_mixtures,\n",
    "    y_model,\n",
    "    dx_labels,\n",
    "    figsize=(14, 6),\n",
    "    save_path='patient_topic_distribution.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Co-Pathology Examples\n",
    "\n",
    "Visualize topic mixtures for individual patients to see co-pathology patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get patient IDs if available\n",
    "patient_ids = df['PTID_ANONY'].values if 'PTID_ANONY' in df.columns else None\n",
    "\n",
    "fig = plot_copathology_mixtures(\n",
    "    patient_mixtures,\n",
    "    y_model,\n",
    "    dx_labels,\n",
    "    patient_ids=patient_ids,\n",
    "    n_patients_per_dx=5,\n",
    "    figsize=(16, 8),\n",
    "    save_path='copathology_examples.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Identify Mixed Pathology Patients\n",
    "\n",
    "Find patients with high entropy (mixed topic membership) indicating co-pathology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "# Compute entropy of topic mixtures\n",
    "topic_entropy = np.array([entropy(patient_mixtures[i]) for i in range(len(patient_mixtures))])\n",
    "\n",
    "# Find patients with highest entropy (most mixed)\n",
    "high_entropy_idx = np.argsort(topic_entropy)[-10:]\n",
    "\n",
    "print(\"Patients with highest co-pathology (mixed topic membership):\\n\")\n",
    "print(f\"{'Patient':<15} {'Diagnosis':<10} {'Entropy':<10} {'Topic Mixture'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx in high_entropy_idx:\n",
    "    patient_id = patient_ids[idx] if patient_ids is not None else f\"Patient_{idx}\"\n",
    "    dx = dx_labels[y_model[idx]]\n",
    "    ent = topic_entropy[idx]\n",
    "    mixture = patient_mixtures[idx]\n",
    "    mixture_str = ' '.join([f\"{m:.2f}\" for m in mixture])\n",
    "    print(f\"{patient_id:<15} {dx:<10} {ent:<10.3f} [{mixture_str}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Topic-Diagnosis Associations\n",
    "\n",
    "Understand how topics predict diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_topic_diagnosis_association(\n",
    "    diagnosis_weights,\n",
    "    dx_labels,\n",
    "    figsize=(9, 6),\n",
    "    save_path='topic_diagnosis_association.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret topic-diagnosis associations\n",
    "print(\"Topic-Diagnosis Associations (η matrix):\\n\")\n",
    "print(f\"{'Topic':<10}\", end=\"\")\n",
    "for dx in dx_labels:\n",
    "    print(f\"{dx:>10}\", end=\"\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "for topic_id in range(model.n_topics):\n",
    "    print(f\"Topic {topic_id:<4}\", end=\"\")\n",
    "    for dx_id in range(len(dx_labels)):\n",
    "        weight = diagnosis_weights[topic_id, dx_id]\n",
    "        print(f\"{weight:>10.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Positive weights: Topic increases probability of diagnosis\")\n",
    "print(\"- Negative weights: Topic decreases probability of diagnosis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Predictions (Optional)\n",
    "\n",
    "Evaluate the model's ability to predict diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict diagnoses for training data\n",
    "y_pred = model.predict_diagnosis(X_model)\n",
    "y_pred_proba = model.predict_diagnosis_proba(X_model)\n",
    "\n",
    "print(f\"Predicted diagnoses shape: {y_pred.shape}\")\n",
    "print(f\"Prediction probabilities shape: {y_pred_proba.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "fig = plot_confusion_matrix(\n",
    "    y_model,\n",
    "    y_pred,\n",
    "    dx_labels,\n",
    "    figsize=(9, 8),\n",
    "    save_path='confusion_matrix.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_model, y_pred, target_names=dx_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Topic Interpretation Guide\n",
    "\n",
    "Use the following analysis to interpret what each topic represents:\n",
    "\n",
    "### Expected Topic Patterns (Examples)\n",
    "\n",
    "**Topic 0: Limbic/Temporal Pattern**\n",
    "- High weights: entorhinal, parahippocampal, fusiform, temporal regions\n",
    "- Associated diagnoses: AD, SVAD (Alzheimer's pathology)\n",
    "\n",
    "**Topic 1: Cortical/Parietal Pattern**\n",
    "- High weights: precuneus, parietal, posterior cingulate\n",
    "- Associated diagnoses: AD, DLB (cortical atrophy)\n",
    "\n",
    "**Topic 2: Subcortical Sparing Pattern**\n",
    "- Lower overall atrophy weights\n",
    "- Associated diagnoses: PD (less cortical involvement)\n",
    "\n",
    "**Topic 3: Minimal Atrophy**\n",
    "- Very low atrophy across regions\n",
    "- Associated diagnoses: HC (healthy controls)\n",
    "\n",
    "### Co-Pathology Interpretation\n",
    "\n",
    "Patients with mixed topic memberships may have:\n",
    "- **AD + DLB**: Mixed limbic and cortical patterns\n",
    "- **SVAD**: AD-like pattern plus vascular features\n",
    "- **PD with cognitive impairment**: PD pattern plus cortical involvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topic patterns\n",
    "topic_df = pd.DataFrame(\n",
    "    topic_patterns.T,\n",
    "    index=feature_names,\n",
    "    columns=[f'Topic_{i}' for i in range(model.n_topics)]\n",
    ")\n",
    "topic_df.to_csv('topic_patterns.csv')\n",
    "print(\"Saved topic patterns to topic_patterns.csv\")\n",
    "\n",
    "# Save patient mixtures\n",
    "mixture_df = pd.DataFrame(\n",
    "    patient_mixtures,\n",
    "    columns=[f'Topic_{i}' for i in range(model.n_topics)]\n",
    ")\n",
    "mixture_df['Diagnosis'] = [dx_labels[i] for i in y_model]\n",
    "if patient_ids is not None:\n",
    "    mixture_df['Patient_ID'] = patient_ids\n",
    "mixture_df.to_csv('patient_topic_mixtures.csv', index=False)\n",
    "print(\"Saved patient mixtures to patient_topic_mixtures.csv\")\n",
    "\n",
    "# Save diagnosis weights\n",
    "dx_weights_df = pd.DataFrame(\n",
    "    diagnosis_weights,\n",
    "    index=[f'Topic_{i}' for i in range(model.n_topics)],\n",
    "    columns=dx_labels\n",
    ")\n",
    "dx_weights_df.to_csv('topic_diagnosis_weights.csv')\n",
    "print(\"Saved diagnosis weights to topic_diagnosis_weights.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading and preprocessing neurodegenerative disease data\n",
    "2. Fitting a supervised LDA model with continuous features\n",
    "3. Extracting and interpreting latent pathology patterns\n",
    "4. Analyzing co-pathology through patient topic mixtures\n",
    "5. Understanding topic-diagnosis associations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Validate convergence**: Check R-hat values and trace plots\n",
    "2. **Sensitivity analysis**: Try different numbers of topics (3-6)\n",
    "3. **Clinical validation**: Compare topics with known pathology patterns\n",
    "4. **Longitudinal analysis**: Apply model to follow-up scans\n",
    "5. **Feature expansion**: Include subcortical structures\n",
    "6. **External validation**: Test on independent dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slda_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
